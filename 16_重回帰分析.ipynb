{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重回帰分析\n",
    "\n",
    "目的変数$y$を説明変数$x_1, x_2, x_3, ..., x_d$の線形関数と定数項で説明するモデルのこと。\n",
    "$$\n",
    "y = B_0 + B_1x_1 + ... + B_dx_d + ε\n",
    "$$\n",
    "ただし、$B_1, B_2, ..., B_d\\in{R}$は各変数の$y$への影響を表現する**回帰係数**で、$B_0$は切片項にあたる。  \n",
    "また、$ε$は誤差項で平均0かつ分散$σ^2$とする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "予測結果：[12.21, 12.7, 12.03, 13.72, 12.91, 13.28, 12.87, 13.43, 13.89, 13.79, 12.93, 12.65, 13.81, 12.62, 12.55, 12.56, 13.36, 12.87, 13.52, 12.2, 12.19, 13.61, 12.35, 12.61, 12.79, 12.61, 14.45, 13.32, 12.79, 13.08, 12.33, 13.5, 13.52, 13.71, 13.14, 12.61, 14.84, 12.49, 12.73, 12.49, 13.51, 13.39, 13.47, 13.21, 12.44, 13.67, 13.56, 13.99, 12.43, 14.17, 13.79, 13.45, 12.16, 12.29, 12.69, 12.93, 12.52, 13.66, 13.33, 13.82, 12.75, 12.39, 13.86, 12.1, 13.17, 12.58, 13.27, 12.61, 12.2, 13.68, 14.37, 12.48, 12.89, 12.44, 13.19, 13.75, 12.51, 12.29]\n",
      "実際の値：[11.82, 11.45, 12.22, 13.74, 13.24, 13.51, 13.73, 14.02, 13.05, 13.83, 13.11, 12.64, 14.21, 12.93, 13.05, 13.03, 12.77, 12.08, 13.17, 11.81, 12.42, 13.39, 11.65, 12.81, 13.05, 13.16, 13.94, 12.85, 11.96, 13.68, 12.29, 14.22, 13.64, 13.83, 13.5, 12.37, 14.19, 13.88, 13.67, 12.47, 14.1, 13.48, 13.71, 13.84, 12.34, 13.9, 13.73, 13.86, 12.7, 14.2, 13.75, 14.16, 12.0, 12.08, 13.62, 13.69, 13.11, 14.38, 13.05, 14.39, 12.85, 11.84, 13.29, 12.52, 12.45, 12.53, 13.87, 12.84, 12.72, 12.25, 14.38, 13.34, 12.72, 11.66, 11.56, 12.82, 12.25, 11.46]\n"
     ]
    }
   ],
   "source": [
    "# wineデータセットで実践\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wine_data = load_wine()\n",
    "wine_df = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)\n",
    "\n",
    "wine_df_train, wine_df_test = train_test_split(wine_df, train_size=100)\n",
    "\n",
    "x_train = wine_df_train.drop('alcohol', axis=1) # 説明変数x1, x2, x3,... ,xd\n",
    "y_train = wine_df_train['alcohol'] # 目的変数y\n",
    "\n",
    "x_test = wine_df_test.drop('alcohol', axis=1)\n",
    "y_test = wine_df_test['alcohol']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "pred_y = model.predict(x_test)\n",
    "\n",
    "print(f\"予測結果：{[round(pred_y[n], 2) for n in range(len(pred_y))]}\")\n",
    "print(f\"実際の値：{y_test.to_list()}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目的変数の変動を説明変数で説明できる部分とそれ以外の部分に分けることで、モデルの説明力を調べることができる。  \n",
    "**決定係数**$R^2$はこの考えを反映させたもので、\n",
    "$$\n",
    "R^2 = \\frac{\\sum_{i=1}^{n} ((x_i-\\bar{x})^T \\hat{\\beta}_{1:d})^2}{\\sum_{i=1}^{n} (y_i-\\bar{y})^2} = \\\n",
    "1-\\frac{\\sum_{i=1}^{n} (y_i-(1,x_i^T)^T \\hat{\\beta})^2}{\\sum_{i=1}^{n} (y_i-\\bar{y})^2}\n",
    "$$\n",
    "と定義される。要するに「$1-($回帰残差$)^2$の総和$/($偏差$)^2$の総和」である。決定係数が大きければそれだけデータのへの当てはまりが良いことを意味する。  \n",
    "なお、決定係数は変数を増やせば増やすほど増大するので、変数の数について調整した**自由度調整済み決定係数**を用いることが多い。  \n",
    "自由度調整済み決定係数$R^{*2}$は、\n",
    "$$\n",
    "R^{*2} = \\frac{\\sum_{i=1}^{n} (y_i-(1,x_i^T)^T \\hat{\\beta})^2/(n-d-1)}{\\sum_{i=1}^{n} (y_i-\\bar{y})^2/(n-1)} = \\\n",
    "1-(1-R^2)\\frac{n-1}{n-d-1}\n",
    "$$\n",
    "で定義される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "決定係数(r2):0.478\n",
      "自由度調整済み決定係数(r2):0.382\n"
     ]
    }
   ],
   "source": [
    "# 決定係数と自由度調整済み決定係数を算出\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(y_test, pred_y)\n",
    "print(f\"決定係数(r2):{round(r2,3)}\")\n",
    "\n",
    "def adj_r2_score(y_true, y_pred, n_dim):\n",
    "    y_true_list = y_true.tolist()\n",
    "    y_pred_list = y_pred.tolist()\n",
    "    data_size = len(y_true_list)\n",
    "\n",
    "    ysos = 0\n",
    "    rsos = 0\n",
    "    y_mean = sum(y_true_list)/len(y_true_list)\n",
    "    for i in range(data_size):\n",
    "        ysos += pow(y_true_list[i]-y_mean, 2)\n",
    "        residual = y_true_list[i] - y_pred_list[i]\n",
    "        rsos += pow(residual, 2)\n",
    "    return 1-(rsos/(data_size-n_dim-1))/(ysos/(data_size-1))\n",
    "\n",
    "adj_r2 = adj_r2_score(y_test, pred_y, len(x_test.columns))\n",
    "print(f\"自由度調整済み決定係数(r2):{round(adj_r2,3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重回帰分析の検定\n",
    "\n",
    "重回帰分析においては、変数の**有意性検定**が重要である。  \n",
    "たとえば、回帰変数$B_k$がゼロか非ゼロかを検定することにより、変数$x_k$が$y$に有意に影響しているかどうかがわかる。  \n",
    "これは推定結果の解釈に重要な示唆を与え、データ分析における強力な解析手法となりうる。  \n",
    "変数の有意性検定を一般化すると、以下のように記述できる。  \n",
    "ある$q<d+1$を満たす$q$に対し、行列$A \\in R^{q(d+1)}$は、ランクが$rank(A)=q$かつ像が$Im(A^T) \\subset Im(X^T)$を満たすとする。  \n",
    "このAを用いて、\n",
    "$$\n",
    "帰無仮説H_0：A\\beta=0、\\quad v.s \\quad\n",
    "対立仮説H_1：A\\beta≠0\n",
    "$$\n",
    "として書ける仮説検定を考える。  \n",
    "たとえば、$A=(0,0,...,1,0,..0)$のように$k+1$番目の成分のみが1のベクトルを考えれば、$A\\beta=0 \\leftrightarrow \\beta_k=0$となる。  \n",
    "このとき、$Θ_0=\\{\\beta \\in \\mathbb{R}^{d+1} | A\\beta=0\\}$として、\n",
    "$$\n",
    "R_0^2 = \\min_{\\beta \\in Θ_0}||Y-X\\beta||^2\n",
    "$$\n",
    "とおき、\n",
    "$$\n",
    "R_1^2 = \\min_{\\beta \\in \\mathbb{R}^{d+1}}||Y-X\\beta||^2\n",
    "$$\n",
    "とおく。  \n",
    "$R_0^2$は帰無仮説のもとで二乗損失がどれだけ小さくできるかを表し、$R_1^2$は対立仮説のもとで二乗損失がどれだけ小さくできるかを表す。  \n",
    "なお$R_1^2=||e||^2$であることには注意。  \n",
    "もし、$R_0^2$と$R_1^2$が大きく変わる場合は、帰無仮説は誤っているとして棄却すればよい。  \n",
    "そこで統計検定量として、\n",
    "$$\n",
    "T = \\frac{(R_0^2-R_1^2)/q}{R_1^2/(n-d-1)}\n",
    "$$\n",
    "を考える。すると帰無仮説のもと、\n",
    "$$\n",
    "T \\sim F(q, n-d-1)\n",
    "$$\n",
    "が成り立つ。なおTは統計量である"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'statsmodels.stats.contrast.ContrastResults'>\n",
       "<F test: F=7.122203894772677, p=4.423342765368874e-08, df_denom=65, df_num=12>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 回帰係数の検定を実行\n",
    "# A=(0,0,...,1,0,..0)(k+1番目の成分のみが1)とする\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "model = sm.OLS(y_test, sm.add_constant(x_test))\n",
    "result = model.fit()\n",
    "A = np.identity(len(result.params))\n",
    "A = A[1:,:]\n",
    "result.f_test(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'statsmodels.stats.contrast.ContrastResults'>\n",
       "<F test: F=113.6426699938149, p=6.585155574954187e-16, df_denom=65, df_num=1>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定数項の検定を実行\n",
    "# A=(1,0,...,0,0,..0)(1番目の成分のみが1)とする\n",
    "A = np.identity(len(result.params))\n",
    "A = A[0,:]\n",
    "result.f_test(A)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正則化\n",
    "\n",
    "重回帰分析では$X^TX$が可逆であることを仮定していたが、高次元データを扱う場合は必ずしもそうなるとは限らない。  \n",
    "そこで$X^TX$の条件数が悪い場合でも安定した推定を行うために**正則化**が有用である。  \n",
    "代表的な正則化手法としては、L1正則化とL2正則化がある。  \n",
    "L1正則化を用いた重回帰はLasso回帰、L2正則化を用いた重回帰はRidge回帰ともよばれる。  \n",
    "正則化を加えることで推定の分散を抑えることができる。  \n",
    "そのため、**学習データ**に合わせすぎて予測精度が悪くなる**過適合**を抑止することができる。  \n",
    "一方で正則化が強すぎると推定量が縮小しすぎて、予測精度が悪化する**過小適合**が発生する点に注意"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1正則化(Lasso回帰)では、正則化項としてのL1ノルムを使用。L1ノルムはベクトル成分の絶対値の和(マンハッタン距離と呼ばれる)である。  \n",
    "回帰係数$\\beta$のいくつかの成分は0(=**スパース**)になり、凸最適化によって変数選択が可能になる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回帰係数\n",
      "                    variable      coef\n",
      "                  malic_acid  0.000000\n",
      "                         ash  0.000000\n",
      "           alcalinity_of_ash -0.000000\n",
      "                   magnesium  0.001028\n",
      "               total_phenols  0.000000\n",
      "                  flavanoids  0.000000\n",
      "        nonflavanoid_phenols -0.000000\n",
      "             proanthocyanins  0.000000\n",
      "             color_intensity  0.119753\n",
      "                         hue -0.000000\n",
      "od280/od315_of_diluted_wines  0.000000\n",
      "                     proline  0.001425\n",
      "決定係数(r2):0.46\n",
      "自由度調整済み決定係数(r2):0.361\n"
     ]
    }
   ],
   "source": [
    "# Lasso回帰を実行\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = Lasso(alpha=0.1)\n",
    "\n",
    "# モデル学習と予測\n",
    "model.fit(x_train, y_train)\n",
    "pred_y = model.predict(x_test)\n",
    "\n",
    "lasso_result_df = pd.DataFrame({\n",
    "    \"variable\":x_train.columns,\n",
    "    \"coef\":model.coef_\n",
    "})\n",
    "print(\"回帰係数\")\n",
    "print(lasso_result_df.to_string(index=False))\n",
    "\n",
    "print(f\"決定係数(r2):{round(r2_score(y_test, pred_y),3)}\")\n",
    "print(f\"自由度調整済み決定係数(r2):{round(adj_r2_score(y_test, pred_y, len(x_test.columns)),3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2正則化(Ridge回帰)では、正則化項としてのL2ノルムを使用。L2ノルムはベクトル成分の絶対値の和(ユークリッド距離と呼ばれる)である。  \n",
    "正則化パラメータを洗濯する基準として**Mallows' CP基準**がある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回帰係数\n",
      "                    variable      coef\n",
      "                  malic_acid  0.084006\n",
      "                         ash -0.069659\n",
      "           alcalinity_of_ash -0.013553\n",
      "                   magnesium  0.001468\n",
      "               total_phenols -0.016127\n",
      "                  flavanoids  0.105805\n",
      "        nonflavanoid_phenols -0.100003\n",
      "             proanthocyanins -0.164003\n",
      "             color_intensity  0.185642\n",
      "                         hue  0.078950\n",
      "od280/od315_of_diluted_wines  0.225571\n",
      "                     proline  0.001030\n",
      "決定係数(r2):0.482\n",
      "自由度調整済み決定係数(r2):0.387\n"
     ]
    }
   ],
   "source": [
    "# Ridge回帰を実行\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge(alpha=1.0)\n",
    "\n",
    "# モデル学習と予測\n",
    "model.fit(x_train, y_train)\n",
    "pred_y = model.predict(x_test)\n",
    "\n",
    "ridge_result_df = pd.DataFrame({\n",
    "    \"variable\":x_train.columns,\n",
    "    \"coef\":model.coef_\n",
    "})\n",
    "\n",
    "print(\"回帰係数\")\n",
    "print(ridge_result_df.to_string(index=False))\n",
    "\n",
    "print(f\"決定係数(r2):{round(r2_score(y_test, pred_y),3)}\")\n",
    "print(f\"自由度調整済み決定係数(r2):{round(adj_r2_score(y_test, pred_y, len(x_test.columns)),3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1正則化とL2正則化を混ぜた**Elastic-Net**と呼ばれる手法もある。  \n",
    "2つの相関が強い変数があるとその2つの変数間で変数選択が安定しないL1正則化の欠点を克服。相関の高い両方の変数を用いる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回帰係数\n",
      "                    variable      coef\n",
      "                  malic_acid  0.000000\n",
      "                         ash  0.000000\n",
      "           alcalinity_of_ash  0.000000\n",
      "                   magnesium  0.001121\n",
      "               total_phenols  0.000000\n",
      "                  flavanoids -0.000000\n",
      "        nonflavanoid_phenols -0.000000\n",
      "             proanthocyanins -0.000000\n",
      "             color_intensity  0.046290\n",
      "                         hue -0.000000\n",
      "od280/od315_of_diluted_wines -0.000000\n",
      "                     proline  0.001599\n",
      "決定係数(r2):0.427\n",
      "自由度調整済み決定係数(r2):0.321\n"
     ]
    }
   ],
   "source": [
    "# Elastic-Netを実行\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "model = ElasticNet(alpha=1.0, l1_ratio=0.5) # l1_ratioはL1ノルムとL2ノルムの比率\n",
    "\n",
    "# モデル学習と予測\n",
    "model.fit(x_train, y_train)\n",
    "pred_y = model.predict(x_test)\n",
    "\n",
    "eln_result_df = pd.DataFrame({\n",
    "    \"variable\":x_train.columns,\n",
    "    \"coef\":model.coef_\n",
    "})\n",
    "\n",
    "print(\"回帰係数\")\n",
    "print(eln_result_df.to_string(index=False))\n",
    "\n",
    "print(f\"決定係数(r2):{round(r2_score(y_test, pred_y),3)}\")\n",
    "print(f\"自由度調整済み決定係数(r2):{round(adj_r2_score(y_test, pred_y, len(x_test.columns)),3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

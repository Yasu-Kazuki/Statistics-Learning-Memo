{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "予測結果：[14.08, 12.01, 13.55, 13.31, 13.35, 13.38, 13.74, 13.8, 12.71, 12.99, 13.39, 12.39, 13.1, 12.47, 13.54, 12.43, 13.01, 13.31, 13.8, 12.2, 13.81, 12.33, 12.68, 13.47, 13.43, 13.77, 12.52, 12.33]\n",
      "実際の値：[13.82, 12.69, 14.34, 14.06, 13.17, 14.02, 14.38, 14.39, 12.37, 13.24, 13.3, 12.17, 13.36, 11.61, 13.83, 12.37, 13.5, 12.77, 14.06, 12.0, 13.75, 11.65, 11.62, 13.9, 13.56, 12.82, 11.87, 13.49]\n"
     ]
    }
   ],
   "source": [
    "# 重回帰分析\n",
    "\n",
    "# 目的変数yを説明変数x1, x2, x3,... ,xdの線形関数と定数項で説明するモデル\n",
    "# y = B0 + B1*x1 + ... + Bd*xd + ε\n",
    "# ただし、B1,B2,..Bd∈Rは各変数のyへの影響を表現する「回帰係数」で、B0は切片項にあたる\n",
    "# また、εは誤差項で平均0かつ分散σ**2とする\n",
    "\n",
    "# 具体例：wineデータセットで実践\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wine_data = load_wine()\n",
    "wine_df = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)\n",
    "\n",
    "wine_df_train, wine_df_test = train_test_split(wine_df, train_size=150)\n",
    "\n",
    "x_train = wine_df_train.drop('alcohol', axis=1) # 説明変数x1, x2, x3,... ,xd\n",
    "y_train = wine_df_train['alcohol'] # 目的変数y\n",
    "\n",
    "x_test = wine_df_test.drop('alcohol', axis=1)\n",
    "y_test = wine_df_test['alcohol']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "pred_y = model.predict(x_test)\n",
    "\n",
    "print(f\"予測結果：{[round(pred_y[n], 2) for n in range(len(pred_y))]}\")\n",
    "print(f\"実際の値：{y_test.to_list()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "決定係数(r2):0.579\n",
      "自由度調整済み決定係数(r2):0.242\n"
     ]
    }
   ],
   "source": [
    "# 目的変数の変動を説明変数で説明できる部分とそれ以外の部分に分けることで、\n",
    "# モデルの説明力を調べることができる。「決定係数」R**2はこの考えを反映させたものである\n",
    "# 決定係数が大きければそれだけデータのへの当てはまりが良いことを意味する\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(f\"決定係数(r2):{round(r2_score(y_test, pred_y),3)}\")\n",
    "\n",
    "# なお、決定係数は変数を増やせば増やすほど増大するので、変数の数について調整した「自由度調整済み決定係数」を用いることが多い\n",
    "\n",
    "def adj_r2_score(y_true, y_pred, n_dim):\n",
    "    y_true_list = y_true.tolist()\n",
    "    y_pred_list = y_pred.tolist()\n",
    "    data_size = len(y_true)\n",
    "    ysos = 0\n",
    "    rsos = 0\n",
    "    y_mean = y_true.mean()\n",
    "    for i in range(data_size):\n",
    "        ysos += (y_mean-y_true_list[i])**2\n",
    "        residual = y_true_list[i] - y_pred_list[i]\n",
    "        rsos += residual**2\n",
    "    \n",
    "    return 1-(rsos/(data_size-n_dim-1))/(ysos/(data_size-1))\n",
    "\n",
    "print(f\"自由度調整済み決定係数(r2):{round(adj_r2_score(y_test, pred_y, len(x_test.columns)),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重回帰分析の検定\n",
    "\n",
    "# 重回帰分析においては、変数の「有意性検定」が重要である。\n",
    "# たとえば、回帰変数Bkがゼロか非ゼロかを検定することにより、変数xkがyに有意に影響しているかどうかがわかる\n",
    "# 変数の有意性検定を一般化すると、以下のように記述できる\n",
    "# あるq<d+1を満たすqに対し、行列A∈R**(q*(d+1))はランクがrank(A)=qかつ像がIm(A**T)⊂Im(X**T)を満たすとする\n",
    "# このAを用いて、帰無仮説H0：A*B=0、対立仮説H1：A*B≠0 として書ける仮説検定を考える\n",
    "# たとえば、A=(0,0,...,1,0,..0)のようにk+1番目の成分のみが1のベクトルを考えれば、A*B=0⇔Bk=0となる\n",
    "# このとき、Θ0={B∈R**(d+1) | A*B=0}として、\n",
    "# R0**2 = min||Y-X*B||**2 (B∈Θ0) とおき\n",
    "# R1**2 = min||Y-X*B||**2 (B∈R*(d+1)) とおく\n",
    "# R0**2は帰無仮説のもとで二乗損失がどれだけ小さくできるかを表し、\n",
    "# R1**2は対立仮説のもとで二乗損失がどれだけ小さくできるかを表す。なおR1**2=||e||**2である\n",
    "# もし、Ro**2とR1**2が大きく変わる場合は、帰無仮説は誤っているとして棄却すればよい\n",
    "# そこで統計検定量として、T = ((Ro**2-R1**2)/q)/(R1**2/(n-d-1)) を考える\n",
    "# すると帰無仮説のもと、T~F(q, n-d-1) が成り立つ。なおTは統計量である"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<F test: F=4.710884220321564, p=0.003004107554451902, df_denom=15, df_num=12>\n"
     ]
    }
   ],
   "source": [
    "# 回帰係数Bk=0(k∈{1,..,d})の検定\n",
    "# A=(0,0,...,1,0,..0)(k+1番目の成分のみが1)とする\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "model = sm.OLS(y_test, sm.add_constant(x_test))\n",
    "result = model.fit()\n",
    "A = np.identity(len(result.params))\n",
    "A = A[1:,:]\n",
    "print(result.f_test(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<F test: F=42.45762290384734, p=9.756380467303627e-06, df_denom=15, df_num=1>\n"
     ]
    }
   ],
   "source": [
    "# 定数項B0=0の検定\n",
    "# A=(1,0,...,0,0,..0)(1番目の成分のみが1)とする\n",
    "\n",
    "A = np.identity(len(result.params))\n",
    "A = A[0,:]\n",
    "print(result.f_test(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正則化\n",
    "\n",
    "# 重回帰分析ではX**T*Xが可逆であることを仮定していたが、高次元データを扱う場合は必ずしもそうなるとは限らない\n",
    "# そこでX**T*Xの条件数が悪い場合でも安定した推定を行うために「正則化」が有用である\n",
    "# 代表的な正則化手法としては、L1正則化とL2正則化がある\n",
    "# L1正則化を用いた重回帰はLasso回帰、L2正則化を用いた重回帰はRidge回帰ともよばれる\n",
    "# 正則化を加えることで推定の分散を抑えることができる。\n",
    "# そのため、「学習データ」に合わせすぎて予測精度が悪くなる「過適合(過学習)」を抑止することができる\n",
    "# 一方で正則化が強すぎると推定量が縮小しすぎて、予測精度が悪化する「過小適合」が発生する点に注意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回帰係数\n",
      "                    variable      coef\n",
      "                  malic_acid  0.033921\n",
      "                         ash  0.000000\n",
      "           alcalinity_of_ash -0.016918\n",
      "                   magnesium -0.001197\n",
      "               total_phenols  0.000000\n",
      "                  flavanoids  0.000000\n",
      "        nonflavanoid_phenols -0.000000\n",
      "             proanthocyanins -0.000000\n",
      "             color_intensity  0.110089\n",
      "                         hue -0.000000\n",
      "od280/od315_of_diluted_wines  0.000000\n",
      "                     proline  0.001288\n",
      "決定係数(r2):0.626\n",
      "自由度調整済み決定係数(r2):0.326\n"
     ]
    }
   ],
   "source": [
    "# L1正則化(Lasso回帰)\n",
    "# 正則化項としてのL1ノルムを使用。ベクトル成分の絶対値の和(マンハッタン距離と呼ばれる)\n",
    "# 回帰係数Bのいくつかの成分は0(=「スパース」)になり、凸最適化によって変数選択が可能になる\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Lasso回帰\n",
    "model = Lasso(alpha=0.1)\n",
    "\n",
    "# モデル学習と予測\n",
    "model.fit(x_train, y_train)\n",
    "pred_y = model.predict(x_test)\n",
    "\n",
    "lasso_result_df = pd.DataFrame({\n",
    "    \"variable\":x_train.columns,\n",
    "    \"coef\":model.coef_\n",
    "})\n",
    "print(\"回帰係数\")\n",
    "print(lasso_result_df.to_string(index=False))\n",
    "\n",
    "print(f\"決定係数(r2):{round(r2_score(y_test, pred_y),3)}\")\n",
    "print(f\"自由度調整済み決定係数(r2):{round(adj_r2_score(y_test, pred_y, len(x_test.columns)),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回帰係数\n",
      "                    variable      coef\n",
      "                  malic_acid  0.146343\n",
      "                         ash  0.205658\n",
      "           alcalinity_of_ash -0.044983\n",
      "                   magnesium -0.001426\n",
      "               total_phenols  0.111888\n",
      "                  flavanoids -0.005925\n",
      "        nonflavanoid_phenols -0.246038\n",
      "             proanthocyanins -0.154032\n",
      "             color_intensity  0.156572\n",
      "                         hue  0.275040\n",
      "od280/od315_of_diluted_wines  0.118168\n",
      "                     proline  0.000914\n",
      "決定係数(r2):0.595\n",
      "自由度調整済み決定係数(r2):0.271\n"
     ]
    }
   ],
   "source": [
    "# L2正則化(Ridge回帰)\n",
    "# 正則化項としてのL2ノルムを使用。ベクトル成分の絶対値の和(ユークリッド距離と呼ばれる)\n",
    "# 正則化パラメータを洗濯する基準として「Mallows' CP基準」がある\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Ridge回帰\n",
    "model = Ridge(alpha=1.0)\n",
    "\n",
    "# モデル学習と予測\n",
    "model.fit(x_train, y_train)\n",
    "pred_y = model.predict(x_test)\n",
    "\n",
    "ridge_result_df = pd.DataFrame({\n",
    "    \"variable\":x_train.columns,\n",
    "    \"coef\":model.coef_\n",
    "})\n",
    "\n",
    "print(\"回帰係数\")\n",
    "print(ridge_result_df.to_string(index=False))\n",
    "\n",
    "print(f\"決定係数(r2):{round(r2_score(y_test, pred_y),3)}\")\n",
    "print(f\"自由度調整済み決定係数(r2):{round(adj_r2_score(y_test, pred_y, len(x_test.columns)),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回帰係数\n",
      "                    variable      coef\n",
      "                  malic_acid  0.000000\n",
      "                         ash  0.000000\n",
      "           alcalinity_of_ash -0.000000\n",
      "                   magnesium -0.000000\n",
      "               total_phenols -0.000000\n",
      "                  flavanoids -0.000000\n",
      "        nonflavanoid_phenols  0.000000\n",
      "             proanthocyanins -0.000000\n",
      "             color_intensity  0.020206\n",
      "                         hue -0.000000\n",
      "od280/od315_of_diluted_wines -0.000000\n",
      "                     proline  0.001540\n",
      "決定係数(r2):0.531\n",
      "自由度調整済み決定係数(r2):0.156\n"
     ]
    }
   ],
   "source": [
    "# Elastic-Net\n",
    "# L1正則化とL2正則化を混ぜたもので、 2つの相関が強い変数があるとその2つの変数間で\n",
    "# 変数選択が安定しないL1正則化の欠点を克服。相関の高い両方の変数を用いる\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Ridge回帰\n",
    "model = ElasticNet(alpha=1.0, l1_ratio=0.5) # l1_ratioはL1ノルムとL2ノルムの比率\n",
    "\n",
    "# モデル学習と予測\n",
    "model.fit(x_train, y_train)\n",
    "pred_y = model.predict(x_test)\n",
    "\n",
    "eln_result_df = pd.DataFrame({\n",
    "    \"variable\":x_train.columns,\n",
    "    \"coef\":model.coef_\n",
    "})\n",
    "\n",
    "print(\"回帰係数\")\n",
    "print(eln_result_df.to_string(index=False))\n",
    "\n",
    "print(f\"決定係数(r2):{round(r2_score(y_test, pred_y),3)}\")\n",
    "print(f\"自由度調整済み決定係数(r2):{round(adj_r2_score(y_test, pred_y, len(x_test.columns)),3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "726ef1321fcd34e1bfe5e95996892c07db726849efbcd4abbb89e2457d1bbe6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
